{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj7qiyFpmKJC841PTTHhiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantiyaramareddy/AI-ML-Learnings/blob/master/ComputerVision/VGGNet_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG-Net Implementation"
      ],
      "metadata": {
        "id": "rxmRmyfkYy-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGGNet is a convolutional neural network architecture proposed by the Visual Geometry Group (VGG) from Oxford University. It is known for its simplicity and uniform architecture, using small convolutional filters and deep layers."
      ],
      "metadata": {
        "id": "w4EF-T6xY5t7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pKDX4KpyYMWT"
      },
      "outputs": [],
      "source": [
        "### Import Required Libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NF7O_ZCiEb4K",
        "outputId": "be92e271-f07e-47e9-9f65-776ca64d0692"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU Availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QZbCSXFdz8I",
        "outputId": "e7b262d1-a9e0-4fcc-d635-ebe850b9a976"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define transforms to be applied on dataset"
      ],
      "metadata": {
        "id": "4xPSmXU_EvNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import padding\n",
        "\n",
        "# Use the mean and sta from CIFAR10\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomCrop(32, padding=4),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = torchvision.transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean, std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "5jlN-GxLE0hm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9dyQEFEFYbc",
        "outputId": "b82c2fa1-fb50-4948-d4ef-c1daaf090033"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:25<00:00, 6.76MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "val_size = 5000\n",
        "train_size = len(train_dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "9xc8AomIFjTk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(test_loader), len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqDe1JeRdMwg",
        "outputId": "dece3cea-d06a-477b-e4a4-e199ac97b965"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(625, 157, 79)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Load Pretrained Model\n",
        "vggnet = models.vgg19(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in vggnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last fully connected layer\n",
        "num_classes = 10\n",
        "vggnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# Only the new final layer should be trainable\n",
        "for param in vggnet.classifier[6].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "vggnet = vggnet.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Rv4lqmdQUM",
        "outputId": "26700c6b-136d-434f-c87a-944f11cde24a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:09<00:00, 63.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vggnet.parameters(), lr=0.001,\n",
        "                       weight_decay=5e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "model = vggnet"
      ],
      "metadata": {
        "id": "jDXNFa6sd7yt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  total_EpochLoss = 0\n",
        "  for images, labels in train_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_EpochLoss += loss.item()\n",
        "\n",
        "avg_loss = total_EpochLoss / len(train_loader)\n",
        "print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)"
      ],
      "metadata": {
        "id": "IqyD14GxePBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}